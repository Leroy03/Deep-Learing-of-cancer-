{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LLevine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXXmgqptHZ1g",
        "outputId": "f1f9d481-3de5-448e-bd24-3cd83c829810"
      },
      "source": [
        "\"\"\"\n",
        "Connect with Google Drive\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8U1ZsiJcZr7"
      },
      "source": [
        "# IF required show the progress bar \n",
        "\n",
        "# !pip3 install tqdm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZzIxHr1Hxlq"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import collections\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader,WeightedRandomSampler,random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "save_folder = '/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/model'\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "import cv2\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd \n",
        "import matplotlib.image as mp\n",
        "import csv\n",
        "from random import shuffle \n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqv5ehXpwA7"
      },
      "source": [
        "# DA\n",
        "\n",
        "\n",
        "> Rotate, Flip, Noise\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dprLkAZiJyLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "e1c5c407-a4b5-4898-bb70-5f2c9c924e1c"
      },
      "source": [
        "\"\"\"\n",
        "Create more data to reduce the imbalance and training data \n",
        "\"\"\"\n",
        "# def sp_noise(image,prob):\n",
        "\n",
        "#     '''\n",
        "#     prob:\n",
        "#     '''\n",
        "\n",
        "#     output = np.zeros(image.shape,np.uint8)\n",
        "\n",
        "#     thres = 1 - prob\n",
        "\n",
        "#     for i in range(image.shape[0]):\n",
        "\n",
        "#         for j in range(image.shape[1]):\n",
        "\n",
        "#             rdn = random.random()\n",
        "\n",
        "#             if rdn < prob:\n",
        "\n",
        "#                 output[i][j] = 0\n",
        "\n",
        "#             elif rdn > thres:\n",
        "\n",
        "#                 output[i][j] = 255\n",
        "\n",
        "#             else:\n",
        "\n",
        "#                 output[i][j] = image[i][j]\n",
        "\n",
        "#     return output\n",
        "\n",
        "# def gasuss_noise(image, mean=0, var=0.001):\n",
        "\n",
        "#     '''\n",
        "#         mean : \n",
        "#         var : \n",
        "#     '''\n",
        "#     image = np.array(image/255, dtype=float)\n",
        "\n",
        "#     noise = np.random.normal(mean, var ** 0.5, image.shape)\n",
        "\n",
        "#     out = image + noise\n",
        "\n",
        "#     if out.min() < 0:\n",
        "\n",
        "#         low_clip = -1.\n",
        "\n",
        "#     else:\n",
        "\n",
        "#         low_clip = 0.\n",
        "\n",
        "#     out = np.clip(out, low_clip, 1.0)\n",
        "\n",
        "#     out = np.uint8(out*255)\n",
        "\n",
        "#     return out\n",
        "\n",
        "# x = ['1']\n",
        "# normal = []\n",
        "# name = []\n",
        "# tmp = 6000\n",
        "# num = 6000\n",
        "# for j in x:\n",
        "#   fh = open('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/Raw_train.csv', 'r')\n",
        "#   fp = open(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train.csv\", \"a\")\n",
        "#   for line in fh:\n",
        "#     line = line.rstrip()\n",
        "#     words = line.split(',')\n",
        "#     normal.append(words[0])\n",
        "#     name.append(words[1])\n",
        "#   idx = 0\n",
        "#   normal = normal[1:]\n",
        "#   name = name[1:]\n",
        "#   for i in normal:\n",
        "#     # if stop >= 144:\n",
        "#     #   break\n",
        "#     # stop += 1\n",
        "\n",
        "#     path = \"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/Raw_train/\"+i+\".png\"\n",
        "#     raw_image = Image.open(path)\n",
        "\n",
        "#     rotate_90 = raw_image.rotate(90)\n",
        "#     rotate_180 = raw_image.rotate(180)\n",
        "#     rotate_270 = raw_image.rotate(270)\n",
        "\n",
        "#     #旋转结合翻转\n",
        "#     flip_vertical_raw = raw_image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "#     flip_vertical_90 = rotate_90.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "#     flip_vertical_180 = rotate_180.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "#     flip_vertical_270 = rotate_270.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "#     img=cv2.imread(path)\n",
        "#     out2 = gasuss_noise(img, mean=0, var=0.001)\n",
        "#     out1 = sp_noise(img, prob=0.02)\n",
        "\n",
        "#     #存储\n",
        "#     flip_vertical_raw.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num)+\".png\")\n",
        "#     flip_vertical_90.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+1)+\".png\")\n",
        "#     flip_vertical_180.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+2)+\".png\")\n",
        "#     flip_vertical_270.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+3)+\".png\")\n",
        "#     mp.imsave('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+str(num+4)+'.png',out1)\n",
        "#     rotate_90.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+5)+\".png\")\n",
        "#     rotate_180.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+6)+\".png\")\n",
        "#     rotate_270.save(\"/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/\"+str(num+7)+\".png\")\n",
        "#     mp.imsave('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+str(num+8)+'.png',out2)\n",
        "#     num+=9\n",
        "\n",
        "    \n",
        "#     for k in range(tmp,num):\n",
        "      \n",
        "#       # 寫入 This is a testing! 到檔案\n",
        "#       print(f\"{k},{name[idx]}\",file=fp)\n",
        "    \n",
        "#     # 關閉檔案\n",
        "#     tmp = num\n",
        "#     idx += 1\n",
        "#   fp.close()\n",
        "#   fh.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCreate more data to reduce the imbalance and training data \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8z6hVm0p_2X"
      },
      "source": [
        "# Parameter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohqb5nG2jMH_"
      },
      "source": [
        "# Parameter\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "MAX_EPOCH = 20\n",
        "\n",
        "RESUME_EPOCH = 0\n",
        "\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "LR = 1e-4\n",
        "\n",
        "\n",
        "BASE = '/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOcx0V9EgJ97"
      },
      "source": [
        "\"\"\"\n",
        "Load trained model while doing the prediction \n",
        "\"\"\"\n",
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint['model']  \n",
        "    model.load_state_dict(checkpoint['model_state_dict'])  \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFYmLXSbvk8X"
      },
      "source": [
        "# Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li9ZNrg1gY-M",
        "outputId": "fb87627f-0913-41ca-db25-72ce3b57befb"
      },
      "source": [
        "\"\"\"\n",
        "Convultional Neual Network \n",
        "Structure:\n",
        "  Convtional \n",
        "  Batch norm\n",
        "  Relu to delete negative value ) x4 times \n",
        "  Linear layer to connect convolutional layer\n",
        "  output a 4 dimension linear year while classfication is 4.\n",
        "\"\"\"\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(3,16,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(16,32,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2,2),\n",
        "\n",
        "        nn.Conv2d(32,64,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(64,128,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2,2),\n",
        "\n",
        "        nn.Conv2d(128,128,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(128,256,3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    self.forwards = nn.Sequential(\n",
        "        nn.Linear(8*8*256,4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096,1024),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(1024,128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128,4),\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.conv(x)\n",
        "    # print(x.shape)\n",
        "    x = x.view(-1,8*8*256)\n",
        "    x = self.forwards(x)\n",
        "    return x\n",
        "\n",
        "model = Net().to('cuda')\n",
        "\n",
        "model\n",
        "print(\"...... Initialize the network done!!! .......\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...... Initialize the network done!!! .......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaN8axevv9vH"
      },
      "source": [
        "# Dataset Dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GSs0OMxqxC"
      },
      "source": [
        "# Create label file of every category \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99YFqfqAwCQo",
        "outputId": "4f2aabc8-77cc-4cf5-dac3-620bf3f52d39"
      },
      "source": [
        "\"\"\"\n",
        "Store the label and the path of the images by different categories\n",
        "And load it into 'data' in numpy arrray type.\n",
        "\"\"\"\n",
        "SIZE = 1024  # Label size means the data of the imgs of each category \n",
        "\n",
        "label = defaultdict(list)\n",
        "with open('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train.csv') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    if row[0].isdigit():\n",
        "      label[row[1]].append(row[0])\n",
        "\n",
        "# Set up a array\n",
        "Connective, Immune, Cancer, Normal = [], [], [], []\n",
        "Connective = np.array(label.get('Connective'))\n",
        "Immune = np.array(label.get('Immune'))\n",
        "Cancer = np.array(label.get('Cancer'))\n",
        "Normal = np.array(label.get('Normal'))\n",
        "Connective_ =  np.random.choice(Connective, size=SIZE)\n",
        "Immune_=np.random.choice(Immune, size=SIZE)\n",
        "Cancer_ = np.random.choice(Cancer, size=SIZE)\n",
        "Normal_ = np.random.choice(Normal, size=SIZE)\n",
        "\n",
        "with open('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/class.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Id','Type'])\n",
        "    for co in Connective_:\n",
        "        writer.writerow(['/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+co+'.png', 0])\n",
        "    for im in Immune_:\n",
        "        writer.writerow(['/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+im+'.png', 1])\n",
        "    for ca in Cancer_:\n",
        "        writer.writerow(['/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+ca+'.png', 2])\n",
        "    for no in Normal_:\n",
        "        writer.writerow(['/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/'+no+'.png', 3])\n",
        "\n",
        "f.close()\n",
        "data = pd.read_csv('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/class.csv')\n",
        "data = data.to_numpy()\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvZpyQR5x09F"
      },
      "source": [
        "# Load label "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5S-nIvHyLez"
      },
      "source": [
        "# Train data and Validate data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USZAISJ9yRpg"
      },
      "source": [
        "\"\"\"\n",
        "Separate data into two data: train and validation data \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "X, y = data[:, 0], data[:, 1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)# 10% validation \n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# images to tenser and normalize \n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "def default_loader(path):\n",
        "    img_pil =  Image.open(path)\n",
        "    # img_pil = img_pil.resize((224,224))\n",
        "    img_tensor = transform(img_pil.convert('RGB'))\n",
        "    # img_tensor = img_pil.ToTensor()\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "class trainset(Dataset):\n",
        "    def __init__(self, file_train, label_train, loader=default_loader):\n",
        "       \n",
        "        self.images = file_train\n",
        "        self.target = label_train\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fn, label = self.images[index], self\n",
        "        img = self.loader(fn)\n",
        "        target = self.target[index]\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self): # will return the images length compare with dataset and dataloader \n",
        "        return len(self.images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2CWjnmnt8yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096952ff-fa51-40fc-db26-07ac3ded9c3d"
      },
      "source": [
        "\"\"\"\n",
        "Train data parameter \n",
        "\n",
        "Batch size\n",
        "epoch\n",
        "iterate number \n",
        "loss_plot: store the loss of train data every epoch \n",
        "accuracy_plot: store the accory of train data every epoch \n",
        "valid_loss_plot: store the loss of valid data every epoch \n",
        "valid_accuracy_plot: store the valid accuracy of valid data every epoch\n",
        " \n",
        "\"\"\"\n",
        "\n",
        "batch_size = BATCH_SIZE\n",
        "\n",
        "max_batch = len(X_train)//batch_size    # how many \n",
        "epoch_size = len(X_train) // batch_size #  how many batch of one epoch\n",
        "print(epoch_size) \n",
        "\n",
        "max_iter = MAX_EPOCH * epoch_size\n",
        "start_iter = RESUME_EPOCH * epoch_size\n",
        "epoch = RESUME_EPOCH\n",
        "\n",
        "\n",
        "# Plot loss, accuracy\n",
        "loss_plot = []\n",
        "accuracy_plot = []\n",
        "valid_loss_plot = []\n",
        "valid_accuracy_plot = []\n",
        "\n",
        "# learning rate \n",
        "lr = LR\n",
        "\n",
        "# optimizer will adjust the learning rate or other hyperparameter \n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM,weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Loss function \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "# criterion = LabelSmoothSoftmaxCE()\n",
        "# criterion = LabelSmoothingCrossEntropy()\n",
        "\n",
        "\n",
        "\n",
        "# load data into dataloader for training or testing\n",
        "\n",
        "train_data  = trainset(X_train, y_train)\n",
        "trainloader = DataLoader(train_data, batch_size,shuffle=True)\n",
        "\n",
        "val_data  = trainset(X_test, y_test)\n",
        "valloader = DataLoader(val_data, batch_size,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxJO3KS_AmMU"
      },
      "source": [
        "## adjust learning rate: become smaller while epoch get higher \n",
        "def adjust_learning_rate(learning_rate, learning_rate_decay, optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR multiplied by learning_rate_decay(set 0.98, usually) every epoch\"\"\"\n",
        "    learning_rate = learning_rate * (learning_rate_decay ** epoch)\n",
        "\n",
        "    # update learning rate in optimizer \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = learning_rate\n",
        "\n",
        "    return learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyVxxw8yv1WR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b49BZlV62omU"
      },
      "source": [
        "\"\"\"\n",
        "check trained model is overfitting or underfitting \n",
        "\"\"\"\n",
        "def validation():\n",
        "  model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "  predicts = []\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss = []\n",
        "    acc = []\n",
        "    for images, labels in valloader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = model(images)\n",
        "        val_loss = criterion(outputs, labels)\n",
        "        prediction = torch.max(outputs.data, 1)[1]\n",
        "        val_correct = (prediction == labels).sum()\n",
        "\n",
        "        val_acc = (val_correct.float()) / len(labels) * 100\n",
        "        loss.append(val_loss.cpu().numpy())\n",
        "        acc.append(val_acc.cpu().numpy())\n",
        "        predicts.append([prediction.tolist()[0], labels.tolist()[0]])\n",
        "    print(f\"Valid Loss: {np.mean(loss)},Valid Acc: {np.mean(acc)}\")\n",
        "  return [np.mean(loss),np.mean(acc)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "n9CAlo18sXjp",
        "outputId": "095a1bcb-837c-4935-ca72-2266c0d9272e"
      },
      "source": [
        "\"\"\"\n",
        "train model \n",
        "\"\"\"\n",
        "total_step = len(trainloader)  # total batch trained \n",
        "curr_lr = LR                   # local learning rate \n",
        "min_valid_loss = np.Inf        # minumn validate loss\n",
        "for epoch in range(1,MAX_EPOCH+1):\n",
        "  for i, (images, labels) in enumerate(trainloader):\n",
        "    model.train()\n",
        "    images = images.cuda()\n",
        "    labels = labels.cuda()\n",
        "    \n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "\n",
        "    # update optimizer and backward loos(BP)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    prediction = torch.max(outputs, 1)[1]\n",
        "    train_correct = (prediction == labels).sum()\n",
        "# #train_correct is longtensor transform to float\n",
        "#     print(train_correct.type())\n",
        "    train_acc = (train_correct.float()) / batch_size * 100\n",
        "\n",
        "    # validation function to print out validate loss and accuracy \n",
        "    val_loss, val_acc = validation()     \n",
        "    print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f} || Acc: {:.4f}%, LR: {:.8f}\"\n",
        "    .format(epoch, MAX_EPOCH, i+1, total_step, loss.item(),train_acc,curr_lr))\n",
        "    \n",
        "    #store the data for plot \n",
        "    accuracy_plot.append(train_acc)\n",
        "    loss_plot.append(loss.item())\n",
        "    valid_loss_plot.append(val_loss)\n",
        "    valid_accuracy_plot.append(val_acc)\n",
        "\n",
        "  # Store model each 5 epoch \n",
        "  if epoch % 5 == 0 and epoch > 0:\n",
        "            checkpoint = {'model': model,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        # 'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'epoch': epoch}\n",
        "            torch.save(checkpoint, os.path.join(save_folder, 'epoch_{}.pth'.format(epoch)))\n",
        "  # Adjust local learning rate \n",
        "  curr_lr = adjust_learning_rate(curr_lr, 0.98, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-eb01a10c0e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmin_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInf\u001b[0m        \u001b[0;31m# minumn validate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_EPOCH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1bb97eb5724c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1bb97eb5724c>\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mimg_pil\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# img_pil = img_pil.resize((224,224))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/train/train/21301.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iGgdCD017Ja"
      },
      "source": [
        "# Predict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9FpTddomaA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7cc5ac51-de25-4c5f-9924-7b67f06f6780"
      },
      "source": [
        "\"\"\"\n",
        "Prediction: do the prediction by the trained model \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
        "def predict(model):\n",
        "  model = load_checkpoint(model)\n",
        "  print('..... Finished loading model! ......')\n",
        "  lab = {3:'Normal',2:'Cancer',1:'Immune',0:'Connective'}\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    pred_list, _id = [], []\n",
        "  print('.....Start predicting image.....')\n",
        "  for i in tqdm(range(10001,10401)):\n",
        "           \n",
        "    _id = np.arange(10001,10401)\n",
        "    path = '/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/test/test/'+str(i)+'.png'\n",
        "    img = Image.open(path).convert('RGB') #from PIL import Image # load images by path \n",
        "    img = transform(img).unsqueeze(0) #transform\n",
        "    \n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        img = img.cuda()\n",
        "    # prediction \n",
        "    with torch.no_grad():\n",
        "      out = model(img)\n",
        "      prediction = torch.argmax(out, dim=1).cpu().item()\n",
        "      pred_list.append(prediction)\n",
        "  pred_list = np.array(pred_list)\n",
        "  print(pred_list)\n",
        "  pred_list_name = []\n",
        "  for i in pred_list:\n",
        "    pred_list_name.append(lab[i])\n",
        "\n",
        "  return _id, pred_list_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b97838cc9bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m transform = transforms.Compose(\n\u001b[0m\u001b[1;32m      7\u001b[0m     [transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb9g9cAWmfgB"
      },
      "source": [
        "\"\"\"\n",
        "Store the prediction in csv \n",
        "\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "\n",
        "trained_model = '/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/model/epoch_20.pth'\n",
        "# print(sum(accuracy_plot)/len(accuracy_plot))\n",
        "_id, pred_list = predict(trained_model)\n",
        "submission = pd.DataFrame({\"Id\": _id, \"Type\": pred_list})\n",
        "print()\n",
        "submission.to_csv('/content/drive/MyDrive/deep-learning-for-msc-coursework-2021/'+ '{}_submission.csv'\n",
        "                      .format('LinLeroy'), index=False, header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMFgxv2IA_9"
      },
      "source": [
        "\"\"\"\n",
        "Plot the loss, accuracy and the validation part \n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *  \n",
        "\n",
        "figure(figsize=(14, 6), dpi=80)\n",
        "x = range(1,total_step*MAX_EPOCH+1)\n",
        "\n",
        "plt.plot(x[::2], accuracy_plot[::2], marker='o', mec='r', mfc='w',label='Accuracy')\n",
        "plt.plot(x[::2], valid_accuracy_plot[::2], marker='.', mec='b', mfc='k',label='Valid Accuracy')\n",
        "plt.legend(loc = 1)\n",
        "plt.xlabel(\"Number\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlOkxlDM1-fw"
      },
      "source": [
        "# Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1drGFuaIaUU"
      },
      "source": [
        "\"\"\"\n",
        "Plot the loss, accuracy and the validation part \n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *  \n",
        "\n",
        "figure(figsize=(14, 6), dpi=80)\n",
        "x = range(1,total_step*MAX_EPOCH+1)\n",
        "\n",
        "plt.plot(x[::2], loss_plot[::2], marker='o', mec='r', mfc='w',label='Loss')\n",
        "plt.plot(x[::2], valid_loss_plot[::2], marker='.', mec='b', mfc='k',label='Valid Loss')\n",
        "plt.legend(loc = 1)\n",
        "plt.xlabel(\"Number\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEggCpD_OKTh"
      },
      "source": [
        "\"\"\"\n",
        "Confusion matrix \n",
        "\"\"\"\n",
        "confusion_pred = []\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "nb_classes = 4\n",
        "\n",
        "# Initialize the prediction and label lists(tensors)\n",
        "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "# load model \n",
        "model = load_checkpoint(trained_model)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(trainloader):\n",
        "        inputs = inputs.cuda()\n",
        "        classes = classes.cuda()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append batch prediction results\n",
        "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
        "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, range(4), range(4))\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "\n",
        "\n",
        "sn.heatmap(df_cm, annot=True,cmap=\"YlGnBu\", annot_kws={\"size\": 16},\n",
        "           xticklabels=['Connective','Immune','Cancer','Normal'],\n",
        "           yticklabels=['Connective','Immune','Cancer','Normal']\n",
        "           ) # font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4eBSPU1Z6nz"
      },
      "source": [
        "\"\"\"\n",
        "Confusion matrix \n",
        "\"\"\"\n",
        "confusion_pred = []\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "nb_classes = 4\n",
        "\n",
        "# Initialize the prediction and label lists(tensors)\n",
        "predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "# load model \n",
        "model = load_checkpoint(trained_model)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(valloader):\n",
        "        inputs = inputs.cuda()\n",
        "        classes = classes.cuda()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append batch prediction results\n",
        "        predlist=torch.cat([predlist,preds.view(-1).cpu()])\n",
        "        lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat=confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "\n",
        "\n",
        "df_cm = pd.DataFrame(conf_mat, range(4), range(4))\n",
        "# plt.figure(figsize=(10,7))\n",
        "sn.set(font_scale=1.4) # for label size\n",
        "sn.color_palette(\"flare\", as_cmap=True)\n",
        "\n",
        "\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},\n",
        "           xticklabels=['Connective','Immune','Cancer','Normal'],\n",
        "           yticklabels=['Connective','Immune','Cancer','Normal']\n",
        "           ) # font size\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mA92Q7WkXJ"
      },
      "source": [
        "# Reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKkhAzBHWo1u"
      },
      "source": [
        "https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
        "\n"
      ]
    }
  ]
}